While we were able to implement a fully functional rock slicing module on Spark by the conclusion of the project, we have also identified a number of areas for future work. First, there are two major aspects of Boon's implementation from \cite{slicing} that were not included in our prototype implementation due to time constraints. The first of these is an optimization on intersection checking using bounding spheres. In our implementation, a linear program needs to be solved every time a check is performed to determine whether or not a discontinuity intersects a rock block and therefore divides that rock block. Boon tries to avoid the relatively expensive process of solving an LP by first efficiently computing spheres that bound both the discontinuity and rock block and then checking to see if the two spheres overlap. If they do not, one can immediately conclude that the joint and block do not overlap, rendering the solution of a linear program unnecessary.

The second feature of Boon's implementation that was not discussed here is the treatment of non-persistent rock joints. In our performance analysis experiments, we assumed that joints were persistent, meaning they cross the entire rock volume and do not start or stop at arbitrary points. It should be noted that we have implemented all of the necessary support for non-persistent discontinuities. Our data structures allow the extent of a discontinuity to be specified, and our intersection checks will respect the shape of a discontinuity. Due to time constraints, we did not have the opportunity to test this aspect of our implementation, and this presents a natural task for future work. In fact, we speculate that our implementation may compare even more favorably with Boon's once we analyze the performance in the presence of non-persistent joints.

Additionally, there is low-hanging fruit in the sense that we have not done any pre-conditioning on our input data. As mentioned previously, the input data should mimic the genesis of discontinuities within the rock mass. However, many of these discontinuities will have similar ages such that the older and more persistent joints can be used to cut the rock volume before seeding the RDD. Sorting the oldest joints based on spatial orientation such that the initial volume is divided into somewhat equal sub-volumes will help maintain load balance across nodes. Having a more balanced load from the outset will limit the amount of shuffling and communication which should have a positive impact on performance.

Our performance experiments have made it clear that much work remains to be done to understand the behavior of our algorithm on Spark. The tests described above demonstrate that issues like the number of partitions and the frequency of repartitioning RDDs to mitigate data skew can have a significant impact on performance. We hope to perform a much more comprehensive suite of experiments in order to gain a clear idea of how our algorithm interacts with Spark and how we may be able to tune certain parameters to achieve optimal performance. We expect that there will be non-trivial interactions between these parameters and other quantities like the number of cores or size of the input data set, and trying to determine an optimal Spark configuration for the rock slicing algorithm could be a project in and of itself. Similarly, because there was limited time to learn Scala and Spark during this project, we expect to become more comfortable with the Spark platform during the course of our future work. We hope that this will allow us to improve our implementation to make a more full and intelligent use of the Spark feature set.

Our final goal is to augment the current rock slicing module with two other modules. The first module will process LiDAR data gathered from field studies in order to produce a rock volume and list of joints. This information can then be fed into our rock slicing algorithm. We do not expect the implementation of LiDAR processing to be particularly difficult, mainly because this will involve tasks that can be easily expressed using the Spark model. This includes processing point-cloud data, computing normal vectors, and performing nearest neighbor searches. Our second planned module will take the output of our rock slicing algorithm and perform discrete element modeling to simulate the dynamics of the rock volume. We expect this to be challenging because it is a communication intensive process, which may make a Spark-based implementation inefficient. The Spark development team is currently working on some communication primitives that may prove useful for this work.

Once these three modules are in place, and once we have improved our implementation to achieve better performance on Spark, our ultimate goal is to make this software openly available to the geotechnics community. We believe that it could offer several advantages over an alternative like that featured in \cite{slicing} such as faster execution, the ability to scale both to large computing clusters and to large input data sizes, and the fact that the implementation will be fully open and free to use, in contrast with many of today's popular commercial engineering software packages.
