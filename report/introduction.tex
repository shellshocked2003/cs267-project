Engineers and scientists are often interested in studying jointed rock masses - a body of rock that is broken into discrete blocks by cracks, fractures and other discontinuities in the formation. The behavior of these blocks can then be modeled using well-known discrete element methods. However, one of the more challenging aspects in this kind of analysis and simulation is the identification of the discrete blocks that will be used to model the actual rock mass. In three-dimensional space, identification of discrete polyhedral blocks is extremely tedious because algorithms need to keep track of the vertices, edges and faces of each block as well as the hierarchy of how these features are connected \cite{slicing}. \par

\cite{slicing} propose a new algorithm that requires knowledge only of the faces of each polyhedral block. The space occupied by each polyhedron can be defined as a set of linear inequalities such that determining joint intersections and contact can be solved using techniques from convex optimization, specifically linear programming. As such, only information on the faces of the polyhedra and discontinuities are required to determine blocking and contact. This is a much simpler alternative compared to previous approaches since it relies on simpler and more robust data structures. After expressing each region as a set of linear inequalities, the algorithm iteratively considers each discontinuity and divides each block that intersects the discontinuity into two child blocks. While these child blocks can be treated independently in future iterations, presenting an excellent opportunity for parallelism, the algorithm instead takes a serial approach. \par

The number of rock blocks in a discrete element analysis can be on the order of tens to hundreds of thousands for real-world scale problems so a serial implementation for block generation can be prohibitively slow. However, the embarrassingly parallel nature of the algorithm lends itself to implimentation on Apache Spark - functions such as \texttt{map} and \texttt{flatMap} can be applied accross the data set in parallel. Additionally, the initial data consists of large LIDAR surveys which will have to be queried multiple times to determine fracture orientations and spacing. Spark distributes the data into sets that are analyzed independently and in parallel, allowing for much faster input processing. \par

The parallel implementation of the \cite{slicing} algorithm on Spark thus forms part of a pipeline - it receives its input from a LIDAR post-processor and pre-processes this data for a DEM analysis. This report presents the implementation of the block generation algorithm on Apache Spark; further work is in-progress to add LIDAR pre-processing functionality in the Spark framework.
